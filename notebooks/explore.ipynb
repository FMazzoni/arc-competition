{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc131ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(\"../arc-prize-2025/arc-agi_training_challenges.json\")\n",
    "\n",
    "data = json.load(file.open())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for k, v in data.items():\n",
    "    print(\"challenge\", k)\n",
    "    # pretty pre\n",
    "    for example in v[\"train\"]:\n",
    "        input = example[\"input\"]\n",
    "        output = example[\"output\"]\n",
    "        print(np.array(input))\n",
    "        print(np.array(output))\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e85863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_competition.task_viewer import view_training_challenge\n",
    "\n",
    "# Quick view of a training challenge\n",
    "view_training_challenge(\"00576224\")\n",
    "\n",
    "# # Or with custom file path\n",
    "# from pathlib import Path\n",
    "\n",
    "# html_view = task_viewer(\"arc-prize-2025/arc-agi_training_challenges.json\", \"00576224\")\n",
    "# display(html_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d41d1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_competition.task_viewer import ARCChallenge, ARCDataLoader, ROOT_DIR\n",
    "\n",
    "file_path = ROOT_DIR / \"arc-prize-2025\" / \"arc-agi_training_challenges.json\"\n",
    "\n",
    "challenge = ARCDataLoader.load_challenge(file_path, \"00576224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arc_competition.agents import create_grid_analysis_agent\n",
    "from arc_competition.schemas import GridAnalysisInput\n",
    "\n",
    "# agent = create_grid_analysis_agent()\n",
    "\n",
    "# user_input = GridAnalysisInput(\n",
    "#     input_grid=challenge.train[0].input,\n",
    "#     output_grid=challenge.train[0].output,\n",
    "# )\n",
    "# output = agent.run(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98521e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb378dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atomic_agents import AgentConfig, AtomicAgent\n",
    "from atomic_agents.context import SystemPromptGenerator\n",
    "from arc_competition.config import config\n",
    "from pydantic import Field\n",
    "from atomic_agents.base.base_io_schema import BaseIOSchema\n",
    "from pydantic import BaseModel\n",
    "from arc_competition.task_viewer import GridExample\n",
    "from arc_competition.task_viewer import Grid\n",
    "from arc_competition.utils import calculate_score, calculate_difference_grid\n",
    "\n",
    "\n",
    "class GridExampleModifiedInput(GridExample):\n",
    "    \"\"\"Extends schema for GridExample.\"\"\"\n",
    "\n",
    "    current: Grid = Field(description=\"Current grid as 2D list of integers\")\n",
    "    difference: Grid = Field(description=\"Difference grid as 2D list of 1s and 0s\")\n",
    "    score: float = Field(description=\"Score of the current grid\")\n",
    "\n",
    "\n",
    "# Grid Analysis Schemas\n",
    "class GridAnalysisInput(BaseIOSchema):\n",
    "    \"\"\"Input schema for ARC grid analysis.\"\"\"\n",
    "\n",
    "    example_grids: list[GridExampleModifiedInput] = Field(\n",
    "        description=\"Example grids containing the input and output grids\"\n",
    "    )\n",
    "\n",
    "\n",
    "class GridAnalysisOutput(BaseIOSchema):\n",
    "    \"\"\"Output schema for ARC grid analysis.\"\"\"\n",
    "\n",
    "    # Overall summary\n",
    "    analysis_summary: str = Field(description=\"Overall summary of the analysis\")\n",
    "\n",
    "    function_to_execute: str = Field(description=\"python function to execute\")\n",
    "\n",
    "\n",
    "def create_generator_agent() -> AtomicAgent[GridAnalysisInput, GridAnalysisOutput]:\n",
    "    \"\"\"Create a grid pattern analysis agent specialized for ARC challenges.\"\"\"\n",
    "\n",
    "    client = config.client\n",
    "    # Create optimized system prompt generator for grid analysis\n",
    "    system_prompt_generator = SystemPromptGenerator(\n",
    "        background=[\n",
    "            \"You are an expert in pattern recognition and visual reasoning, specialized in analyzing grid-based puzzles.\",\n",
    "            \"You have deep expertise in ARC (Abstraction and Reasoning Corpus) challenges and can identify subtle patterns.\",\n",
    "            \"You understand both low-level pixel patterns and high-level conceptual transformations.\",\n",
    "            \"Given an input grid, current grid and an output grid, you can identify the next best function to execute to transform the current grid into the output grid.\",\n",
    "            \"Only use small python functions, preferably less than 10 lines of code but no more than 50 lines of code.\",\n",
    "            \"You will also be given a score for the current grid, you can use this to identify issues with the functions or the order of execution.\",\n",
    "            \"The score is a float between 0 and 100, where 0 is the worst and 100 is the best.\",\n",
    "            \"50 points are deducted for mismatch output sizes between the current grid and the output grid.\",\n",
    "            \"1 point is deducted for each element in the current grid that is unmatched in the output grid.\",\n",
    "            \"all comparisons will be done with the top left element of the current grid as the reference point.\",\n",
    "            \"a reference difference grid will be provided to you, you can use this to identify the differences between the current grid and the output grid.\",\n",
    "            \"The reference difference grid will be a 2D grid of 0s and 1s, where 0s are the elements that are matched and 1s are the elements that are unmatched.\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create the grid analysis agent\n",
    "    grid_agent = AtomicAgent[GridAnalysisInput, GridAnalysisOutput](\n",
    "        config=AgentConfig(\n",
    "            client=client,\n",
    "            model=config.model.name,\n",
    "            system_prompt_generator=system_prompt_generator,\n",
    "            model_api_parameters=config.model.get_api_parameters(),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return grid_agent\n",
    "\n",
    "\n",
    "def update_to_modified_input(input: Grid, output: Grid) -> GridExampleModifiedInput:\n",
    "    return GridExampleModifiedInput(\n",
    "        input=input,\n",
    "        output=output,\n",
    "        current=input,\n",
    "        score=calculate_score(input, output),\n",
    "        difference=calculate_difference_grid(input, output),\n",
    "    )\n",
    "\n",
    "\n",
    "examples = [\n",
    "    update_to_modified_input(example.input, example.output)\n",
    "    for example in challenge.train\n",
    "]\n",
    "\n",
    "user_input = GridAnalysisInput(\n",
    "    example_grids=examples,\n",
    ")\n",
    "\n",
    "grid_agent = create_generator_agent()\n",
    "output = grid_agent.run(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdede0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.function_to_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b36abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_grid(grid):\n",
    "    # Get the original 2x2 grid\n",
    "    original = [row[:] for row in grid]\n",
    "\n",
    "    # Create a flipped version (swap rows)\n",
    "    flipped = [original[1], original[0]]\n",
    "\n",
    "    # Create the 6x6 output grid\n",
    "    output = []\n",
    "\n",
    "    # First two rows: repeat original horizontally 3 times\n",
    "    for row in original:\n",
    "        output.append(row * 3)\n",
    "\n",
    "    # Next two rows: repeat flipped horizontally 3 times\n",
    "    for row in flipped:\n",
    "        output.append(row * 3)\n",
    "\n",
    "    # Last two rows: repeat original horizontally 3 times again\n",
    "    for row in original:\n",
    "        output.append(row * 3)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "llm_output = transform_grid(challenge.train[1].input)\n",
    "llm_output = Grid(llm_output)\n",
    "diff_grid = calculate_difference_grid(llm_output, challenge.train[1].output)\n",
    "calculate_score(llm_output, challenge.train[1].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf7f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge.train[0].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36322be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.function_to_execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f012b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(challenge.train[0].prompt_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc0cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
